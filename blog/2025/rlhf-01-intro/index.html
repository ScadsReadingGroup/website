<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Learning to Summarize from Human Feedback | Reading Group Scads.AI </title> <meta name="author" content="blank blank blank"> <meta name="description" content="This post summarizes the paper " learning to summarize from human feedback> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://scadsreadinggroup.github.io/blog/2025/rlhf-01-intro/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Reading Group Scads.AI </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Papers </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Learning to Summarize from Human Feedback</h1> <p class="post-meta"> Created on March 14, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/rlhf"> <i class="fa-solid fa-hashtag fa-sm"></i> rlhf</a>   ·   <a href="/blog/category/paper-summary"> <i class="fa-solid fa-tag fa-sm"></i> paper-summary</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="presented-by-andrei-politov">Presented by Andrei Politov</h2> <div class="row mt-3 justify-content-center"> <div class="col-sm-auto mt-3 mt-md-0"> <figure> <audio src="/assets/audio/rlhf-01.mp3" controls=""></audio> </figure> </div> <div class="col-sm-auto mt-3 mt-md-0"> <figure> <audio src="/assets/audio/rlhf-01-podcast.wav" controls=""></audio> </figure> </div> </div> <div class="caption"> Listen to the audio recording of the meeting on the left. You can also listen to the podcast generated from the paper and the meeting notes on the right. </div> <p><strong>Overall Theme:</strong> The meeting centered on OpenAI’s 2022 paper, “Learning from Human Feedback” (RLHF), highlighting its groundbreaking approach to training language models to align better with human preferences for text summarization. The discussion emphasized the paper’s practical impact and its core methodologies.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/_page_3_Figure_0-480.webp 480w,/assets/img/_page_3_Figure_0-800.webp 800w,/assets/img/_page_3_Figure_0-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/_page_3_Figure_0.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Key Takeaways:</strong></p> <ul> <li> <strong>Retrospective Significance:</strong> The paper is viewed as highly representative of advancements in the field, particularly in how it presaged the techniques employed in models like ChatGPT.</li> <li> <strong>Core Idea:</strong> The central concept is to use human feedback to train a reward model that guides the reinforcement learning process to generate summaries preferred by humans. This contrasts with traditional supervised learning methods where models are trained to mimic existing human-written summaries.</li> <li> <strong>Human Feedback is Crucial:</strong> The precision and care taken in collecting and using human annotations are identified as key to the paper’s success. The model learns what humans like and dislike.</li> <li> <strong>Larger Models:</strong> The success hinges partly on the use of larger models, combined with RL and careful task formulation. GPT-3 was used for everything.</li> <li> <p><strong>Workflow:</strong></p> <ol> <li> <strong>Data Collection (TL;DR Dataset from Reddit):</strong> Original posts from Reddit and summaries written by the original authors were used.</li> <li> <strong>Summary Generation:</strong> Multiple summaries are generated using various policies (models). These are initially GPT models pre-trained on Wikipedia data.</li> <li> <strong>Human Evaluation:</strong> Humans compare pairs of summaries for a given post, indicating which is better.</li> <li> <strong>Reward Model Training:</strong> A reward model is trained on the human feedback data to predict the quality of summaries. The model calculates a reward for each summary, learning to distinguish between “good” and “bad” summaries.</li> <li> <strong>Policy Training (PPO):</strong> The policy (the summarization model) is fine-tuned using Proximal Policy Optimization (PPO). The reward model provides feedback on the generated summaries, guiding the policy to produce better outputs. The trained policy generates summaries based on new posts, and the reward model scores them. This process is reiterated to improve the model.</li> <li> <strong>Fine-tuning:</strong> The model is fine-tuned based on the human feedback.</li> </ol> </li> <li> <strong>Contradiction of Supervised Methods:</strong> Traditional supervised learning models often contradict what is actually desired – summaries that are human-like and prioritize quality.</li> <li> <strong>Superior Performance:</strong> Models trained using human feedback significantly outperform those trained solely with supervised learning, even when using larger supervised learning models. The model can even surpass the quality of original human summaries in the dataset.</li> <li> <strong>RL vs. Next Token Prediction:</strong> A major discussion point was the difference between RL-based fine-tuning and simple supervised fine-tuning using next-token prediction. The key difference lies in the objective/loss function, and the fact that RL can better align with human expectations. Next token prediction trains the model to be very exact in this task. RL is more effective because you don’t need to write exact answers to prompts.</li> <li> <strong>Role of the Reward Model:</strong> The reward model judges entire texts, not just next tokens. It acts as a “supervised human” but in model form, enabling cheaper and larger-scale evaluation of summaries.</li> <li> <strong>“Policy” Defined:</strong> In this context, “policy” refers to the generative model being used for summarization.</li> <li> <strong>ROUGE Metric Inadequacy:</strong> The meeting participants noted that the ROUGE metric is not a reliable indicator of human-like behavior.</li> <li> <strong>Dataset Significance:</strong> The use of the Reddit dataset was a crucial decision, contributing to the generalizability of the model to news datasets without specific fine-tuning.</li> <li> <strong>Darmstadt Work:</strong> Research from Darmstadt was mentioned, highlighting similar work being done concurrently, but with different datasets and a focus on error investigation.</li> </ul> <p><strong>Points of Discussion/Questions:</strong></p> <ul> <li>Clarification on the difference between RL and next token prediction.</li> <li>The meaning of the term “policy” within the context of the paper.</li> </ul> <p><strong>Key Quotes/Observations:</strong></p> <ul> <li>“They had ideas that they implemented in the chat GPT or whatever we have now in this paper, actually.”</li> <li>“Another thing that they used much larger models here, and that’s probably synergy of using large models, this well, not well, but in the good minor research, their reinforcement learning techniques and right formulation of tasks.”</li> <li>“…summaries from our human feedback models are preferred by our labelers to the original human demonstrations in the dataset, which I find very interesting.”</li> </ul> <p><strong>Conclusion:</strong></p> <p>The meeting provided a deep dive into the core concepts and implications of the “Learning from Human Feedback” paper. It emphasized the power of RLHF in creating language models that are better aligned with human preferences and capable of generating higher-quality summaries than traditional supervised learning approaches. The discussion highlighted the significance of the human feedback process, the role of the reward model, and the limitations of traditional evaluation metrics like ROUGE.</p> <p><strong>Appendix</strong></p> <p>Some figures from the paper.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/graph-of-thoughts/">Graph of Thoughts</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 blank blank blank. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>