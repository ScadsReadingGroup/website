---
layout: post
title: Learning to Summarize from Human Feedback
date: 2025-03-14 17:45
description: This post summarizes the paper "Learning to Summarize from Human Feedback"
tags: rlhf
categories: paper-summary
---

## Presented by Andrei Politov

<div class="row mt-3 justify-content-center">
    <div class="col-sm-auto mt-3 mt-md-0">
        {% include audio.liquid path="assets/audio/rlhf-01.mp3" controls=true %}
    </div>
    <div class="col-sm-auto mt-3 mt-md-0">
        {% include audio.liquid path="assets/audio/rlhf-01-podcast.wav" controls=true %}
    </div>
</div>
<div class="caption">
    Listen to the audio recording of the meeting on the left.
    You can also listen to the podcast generated from the paper and the meeting notes on the right.

</div>


**Overall Theme:** The meeting centered on OpenAI's 2022 paper, "Learning from Human Feedback" (RLHF), highlighting its groundbreaking approach to training language models to align better with human preferences for text summarization. The discussion emphasized the paper's practical impact and its core methodologies.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/_page_3_Figure_0.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

**Key Takeaways:**

*   **Retrospective Significance:** The paper is viewed as highly representative of advancements in the field, particularly in how it presaged the techniques employed in models like ChatGPT.
*   **Core Idea:** The central concept is to use human feedback to train a reward model that guides the reinforcement learning process to generate summaries preferred by humans. This contrasts with traditional supervised learning methods where models are trained to mimic existing human-written summaries.
*   **Human Feedback is Crucial:** The precision and care taken in collecting and using human annotations are identified as key to the paper's success.  The model learns what humans like and dislike.
*   **Larger Models:**  The success hinges partly on the use of larger models, combined with RL and careful task formulation. GPT-3 was used for everything.
*   **Workflow:**

    1.  **Data Collection (TL;DR Dataset from Reddit):** Original posts from Reddit and summaries written by the original authors were used.
    2.  **Summary Generation:** Multiple summaries are generated using various policies (models). These are initially GPT models pre-trained on Wikipedia data.
    3.  **Human Evaluation:** Humans compare pairs of summaries for a given post, indicating which is better.
    4.  **Reward Model Training:** A reward model is trained on the human feedback data to predict the quality of summaries.  The model calculates a reward for each summary, learning to distinguish between "good" and "bad" summaries.
    5.  **Policy Training (PPO):** The policy (the summarization model) is fine-tuned using Proximal Policy Optimization (PPO). The reward model provides feedback on the generated summaries, guiding the policy to produce better outputs. The trained policy generates summaries based on new posts, and the reward model scores them. This process is reiterated to improve the model.
    6.  **Fine-tuning:** The model is fine-tuned based on the human feedback.
*   **Contradiction of Supervised Methods:**  Traditional supervised learning models often contradict what is actually desired â€“ summaries that are human-like and prioritize quality.
*   **Superior Performance:**  Models trained using human feedback significantly outperform those trained solely with supervised learning, even when using larger supervised learning models. The model can even surpass the quality of original human summaries in the dataset.
*   **RL vs. Next Token Prediction:** A major discussion point was the difference between RL-based fine-tuning and simple supervised fine-tuning using next-token prediction.  The key difference lies in the objective/loss function, and the fact that RL can better align with human expectations. Next token prediction trains the model to be very exact in this task. RL is more effective because you don't need to write exact answers to prompts.
*   **Role of the Reward Model:** The reward model judges entire texts, not just next tokens.  It acts as a "supervised human" but in model form, enabling cheaper and larger-scale evaluation of summaries.
*   **"Policy" Defined:** In this context, "policy" refers to the generative model being used for summarization.
*   **ROUGE Metric Inadequacy:** The meeting participants noted that the ROUGE metric is not a reliable indicator of human-like behavior.
*   **Dataset Significance:** The use of the Reddit dataset was a crucial decision, contributing to the generalizability of the model to news datasets without specific fine-tuning.
*   **Darmstadt Work:** Research from Darmstadt was mentioned, highlighting similar work being done concurrently, but with different datasets and a focus on error investigation.

**Points of Discussion/Questions:**

*   Clarification on the difference between RL and next token prediction.
*   The meaning of the term "policy" within the context of the paper.

**Key Quotes/Observations:**

*   "They had ideas that they implemented in the chat GPT or whatever we have now in this paper, actually."
*   "Another thing that they used much larger models here, and that's probably synergy of using large models, this well, not well, but in the good minor research, their reinforcement learning techniques and right formulation of tasks."
*   "...summaries from our human feedback models are preferred by our labelers to the original human demonstrations in the dataset, which I find very interesting."

**Conclusion:**

The meeting provided a deep dive into the core concepts and implications of the "Learning from Human Feedback" paper. It emphasized the power of RLHF in creating language models that are better aligned with human preferences and capable of generating higher-quality summaries than traditional supervised learning approaches. The discussion highlighted the significance of the human feedback process, the role of the reward model, and the limitations of traditional evaluation metrics like ROUGE.

**Appendix**

Some figures from the paper.

