---
layout: post
title: Graph of Thoughts
date: 2025-03-14 17:45
description: This post summarizes the paper "Graph of Thoughts"
tags: reasoning, MCTS, self-refine
categories: paper-summary
---


## Presented by Lalith Manjunath
<div class="row mt-3 justify-content-center">
    <div class="col-sm-auto mt-3 mt-md-0">
        {% include audio.liquid path="assets/audio/graph-of-thoughts.mp3" controls=true %}
    </div>
    <div class="col-sm-auto mt-3 mt-md-0">
        {% include audio.liquid path="assets/audio/graph-of-thoughts-podcast.wav" controls=true %}
    </div>
</div>
<div class="caption">
    Listen to the audio recording of the meeting on the left.
    You can also listen to the podcast generated from the paper and the meeting notes on the right.

</div>

The discussion revolves around the paper on Self-Refine, which utilizes a concept called Monte Carlo Tree Search (MCTS) and contrasts it with other tree traversal methods like A*, DFS, and BFS. The primary goal of MCTS, as applied in this paper, is to iteratively improve the solution generated by a Large Language Model (LLM) rather than arriving at a final solution in a single step. The application is solving tasks that require multi-step reasoning.

**Monte Carlo Tree Search (MCTS) Explained**

*   **Basic Idea:** MCTS is a tree search algorithm that doesn't require a heuristic function. It simulates the environment by performing "rollouts," which are sequences of random actions (in this context, generating text with an LLM) to estimate the value of different states in the tree.
*   **Rollout:** A rollout involves starting from a node (representing a partial solution), randomly selecting actions, and continuing until a terminal condition is met. The state at termination is assigned a value.
*   **Key Steps:**
    *   **Selection:** Starting from the root node (initial state), navigate through promising child nodes.
    *   **Expansion:** Add new nodes to the tree, representing different possible actions or decisions.
    *   **Simulation:** For each new node, run a rollout (randomly sampling actions) to a terminal state.
    *   **Backpropagation:** Propagate the value from the simulation back to the parent nodes. This updates the value estimates of the nodes along the path.
*   **Reinforcement learning** is discussed.
* The application to the board game "Go" and use by AlphaGo.
*   **Monte Carlo:** The term "Monte Carlo" refers to the random nature of the rollouts. It relies on probability distributions rather than a fixed sequence of actions.

**Upper Confidence Bound for Trees (UCT) Value**

*   A crucial concept in MCTS is the UCT value, which helps balance exploration (trying new actions) and exploitation (choosing actions with high estimated values).
*   The UCT value for a node is a sum of two terms:
    *   One term, Xj, represents the average reward (from rollouts) of taking a particular action (exploitation).
    *   The other term represents the exploration bonus, which is higher for nodes that haven't been visited much (exploration). It uses the logarithm of the number of times the parent node has been visited divided by the number of times the child node has been visited.
*   A constant 'C' balances the exploration and exploitation terms.

**Self-Refine**

*   **Core Idea:** Iteratively refine the LLM's output. Instead of generating the complete answer in one go, the model generates partial solutions and refines them in subsequent steps.
*   **Chain of Thought (COT):**
    *   A technique where the LLM is prompted to generate intermediate reasoning steps, leading to the final answer. The presenter compares this to how humans solve complex problems by breaking them down into smaller steps.
*   **Multiple COTs:** Run multiple (K) chains of thought in parallel and aggregate the results. This is analogous to a random forest approach, increasing confidence by averaging over multiple runs.
*   **Self-Consistency:** The idea is that even with different random rollouts, the model should converge towards a consistent answer.

**Graph of Thoughts**

*   The paper "Graph of Thoughts: Solving Elaborate Problems with LLMs" proposes that the model operates on a graph instead of a simple tree, allowing for more complex relationships between nodes.
    * It introduces operations like split (branching into multiple possibilities) and merge (combining results from different branches).
    * It is proposed to be useful for problems where you can represent relationships between different solution steps.

**Applications and Examples**

*   **Math Word Problems:** The paper focuses on solving mathematical reasoning problems, using the `MATH` dataset, which consists of problems written in LaTeX.
*   **Game Playing (AlphaGo):** MCTS was popularized by its use in AlphaGo, where it helps to pick the best move by simulating future game states.
*   **Code Generation (AlphaCode):** AlphaCode, also from DeepMind, generates multiple code solutions for a given problem and uses testing to select the best one.
*   **Tinygrad:** A library similar to PyTorch that uses MCTS to optimize kernel shapes for different hardware.
* Path planning.

**Limitations and Open Questions**

*   **Formal Verification:** The effectiveness of self-refine and MCTS relies on being able to formally evaluate the correctness of a (partial) solution. This is easier for domains like math and code (where you can use theorem provers or unit tests) but harder for more open-ended tasks.
*   **Optimal Representation:** It's unclear whether the specific chain-of-thought steps or graph structures used are the most efficient way to represent the problem-solving process.
*   **Coverage vs. Precision:** The paper discusses the trade-off between coverage (exploring a large portion of the solution space) and precision (identifying the correct solution among the explored possibilities).
*   **Human-like Reasoning:** The presenters discuss the idea that breaking problems down into smaller steps, as in chain-of-thought, might resemble human reasoning, but it's not necessarily how LLMs internally work.
* It is noted that humans do not learn like this in school.

**Key Takeaways**

*   MCTS is a powerful technique for solving problems that can be represented as a search over a tree or graph of states.
*   Self-refine uses MCTS to iteratively improve LLM outputs, leading to more accurate solutions, especially for complex reasoning tasks.
*   Chain-of-thought prompting and the use of multiple parallel chains are important strategies for leveraging LLMs' capabilities.
*   The effectiveness of these methods depends on being able to evaluate intermediate steps or states, which might be more challenging for some domains than others.
* Zipf's law as it related to randomly generated language.